{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8735a494-c42f-4bb5-b649-23edf4319abb",
   "metadata": {},
   "source": [
    "## Data prep and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192feb6c-3838-4753-b4b0-22b80be73ef4",
   "metadata": {},
   "source": [
    "### Data config and result naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c3750a-c58f-457b-9a8a-ce0165dcf644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rad5cds_local.max.ds2to11_adaptive12n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads_dataset_name = \"hsa.dRNASeq.HeLa.polyA.REL5.long.1\"\n",
    "# reads_dataset_name = \"hsa.dRNASeq.HeLa.polyA.REL5.long.2\"\n",
    "reads_dataset_name = \"ds2to11\"\n",
    "\n",
    "reads_dataset_preprocessing = 'cds_local.max.ds2to11'\n",
    "# reads_dataset_preprocessing = 'original'\n",
    "addition = \"adaptive\"\n",
    "\n",
    "SEQ_LENGTH = 12 # after processing it will actually be == SEQ_LENGTH + 1 \n",
    "\n",
    "folder_run_id = 'rad5' + reads_dataset_preprocessing + \"_\" + addition + \"_wd0.01_\" + str(SEQ_LENGTH + 1) + 'n'\n",
    "\n",
    "dataset_save_name = 'rad5' + reads_dataset_preprocessing + \"_\" + addition\n",
    "if SEQ_LENGTH != 200:\n",
    "    dataset_save_name = dataset_save_name + str(SEQ_LENGTH) + 'n'\n",
    "dataset_save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01c9107-1c88-4892-ae3d-d27bd904a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/decay2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import comet_ml\n",
    "from torch import cuda\n",
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98e7ce2-1d15-4dee-ab0c-f13d2fab86d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Comet API key is valid\n",
      "COMET INFO: Comet API key saved in /home/jovyan/.comet.config\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['COMET_API_KEY'] = \"EpKIINrla6U4B4LJhd9Sv4i0b\"\n",
    "\n",
    "# Commet Init\n",
    "comet_ml.init(project_name=\"decay\", api_key= \"EpKIINrla6U4B4LJhd9Sv4i0b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26673699-1813-47b8-869b-2293fa32cbfd",
   "metadata": {},
   "source": [
    "### Func to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f58977-7639-4acb-89c0-94f59dac6be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 01:30:11.585036: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from raw_to_input import *\n",
    "from raw_to_input import reformat, parse_data, parse_neg\n",
    "\n",
    "\n",
    "def load_data(upstream5p, downstream3p, within, rbpfile, negfile, rep, onehot):\n",
    "    \"\"\"Parses and processes data from input FASTA and BED files and returns it in train and testing portions.\n",
    "    @param upstream5p\n",
    "    \"\"\"\n",
    "    # I do not use the RBPs now, but leaving the code here for later\n",
    "    rbp_dict = get_rbp_dict(rbpfile)\n",
    "\n",
    "    x, rbps, y = [], [], []\n",
    "    # Iterate through files and append sequences and corresponding labels and region information to above data arrays\n",
    "    parse_data(upstream5p, x, y, rbps, rbp_dict, 5, rep, onehot, length=SEQ_LENGTH)\n",
    "    print(len(y), ' processed upstream5p; ', 'currect label balance: ', y.count(1), ' ones, ', y.count(0), ' zeros')\n",
    "    parse_data(downstream3p, x, y, rbps, rbp_dict, 3, rep, onehot, length=SEQ_LENGTH)\n",
    "    print(len(y), ' processed downstream3p; ', 'currect label balance: ', y.count(1), ' ones, ', y.count(0), ' zeros')\n",
    "    parse_data(within, x, y, rbps, rbp_dict, \"w\", rep, onehot, length=SEQ_LENGTH)\n",
    "    print(len(y), ' processed within; ', 'currect label balance: ', y.count(1), ' ones, ', y.count(0), ' zeros')\n",
    "\n",
    "    x_neg, rbps_neg, y_neg = [], [], []\n",
    "    \n",
    "    # generate negatives\n",
    "    # in each cycle there we add 1 random positive\n",
    "    while y.count(1) > y_neg.count(0):\n",
    "        parse_neg(negfile, x_neg, y_neg, rbps_neg, rbp_dict, rep, onehot, length=SEQ_LENGTH)\n",
    "        print(len(y_neg), ' processed negfile; ', 'current label balance: ', y.count(1), ' ones, ', y_neg.count(0), ' zeros')\n",
    "    \n",
    "    # combine the positives with the generated negatives\n",
    "    x.extend(x_neg[:len(y)])    \n",
    "    rbps.extend(rbps_neg[:len(y)])  \n",
    "    y.extend(y_neg[:len(y)])\n",
    "    print(len(y), ' total; ', 'label balance: ', y.count(1), ' ones, ', y.count(0), ' zeros')\n",
    "    \n",
    "    return x, rbps, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916588b-1fea-44ee-a00a-cb3576b7e2fd",
   "metadata": {},
   "source": [
    "### Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96aafaab-a22e-452d-85f9-4125cc3eb55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '/home/jovyan'\n",
    "pos_5p_train = home_path + '/decay/decay/get_fasta/results/{}/{}/decay_seqs_fasta/upstream-5P-end-transcripts_fasta_train.fa'.format(reads_dataset_name, reads_dataset_preprocessing + \"/\" + addition)\n",
    "pos_5p_test = home_path + '/decay/decay/get_fasta/results/{}/{}/decay_seqs_fasta/upstream-5P-end-transcripts_fasta_test.fa'.format(reads_dataset_name, reads_dataset_preprocessing + \"/\" + addition)\n",
    "\n",
    "pos_3p_train = home_path + '/decay/decay/get_fasta/results/{}/{}/decay_seqs_fasta/downstream-3P-end-transcripts_fasta_train.fa'.format(reads_dataset_name, reads_dataset_preprocessing + \"/\" + addition)\n",
    "pos_3p_test = home_path + '/decay/decay/get_fasta/results/{}/{}/decay_seqs_fasta/downstream-3P-end-transcripts_fasta_test.fa'.format(reads_dataset_name, reads_dataset_preprocessing + \"/\" + addition)\n",
    "\n",
    "within_train = home_path + '/decay/decay/get_fasta/results/{}/{}/decay_seqs_fasta/within_transcripts_fasta_train.fa'.format(reads_dataset_name, reads_dataset_preprocessing + \"/\" + addition)\n",
    "within_test = home_path + '/decay/decay/get_fasta/results/{}/{}/decay_seqs_fasta/within_transcripts_fasta_test.fa'.format(reads_dataset_name, reads_dataset_preprocessing + \"/\" + addition)\n",
    "\n",
    "neg_file = home_path + '/decay/decay/data/hg38/transcripts.fa'\n",
    "\n",
    "rbp_file = home_path + '/decay/decay/rbp_data/AATF.bed'\n",
    "\n",
    "rep = False # was True --- switch for soft-masked and hard-masked nucleotides \n",
    "onehot = False # Return nucleotide characters or one-hot vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f1c198-5f18-461e-99ec-b7415b156490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/decay/decay/get_fasta/results/ds2to11/cds_local.max.ds2to11/adaptive/decay_seqs_fasta/upstream-5P-end-transcripts_fasta_train.fa'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_5p_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955abea1-603e-4d69-b64d-3a0ca56ea8f7",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f8d1aa-5c31-4e03-89e1-9b2d5a00264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dataset_save_name):\n",
    "    # Load train data\n",
    "    x_train_raw, rbps_train, y_train = load_data(\n",
    "        pos_5p_train, pos_3p_train, within_train, \n",
    "        rbp_file, neg_file, rep, onehot)\n",
    "\n",
    "    # Load test data\n",
    "    x_test_raw, rbps_test, y_test = load_data(\n",
    "        pos_5p_test, pos_3p_test, within_test, \n",
    "        rbp_file, neg_file, rep, onehot)\n",
    "\n",
    "\n",
    "\n",
    "#     if onehot:\n",
    "#         n_seqlength, n_features = x_train_raw[0].shape[0], x_train_raw[0].shape[1]\n",
    "#     else:\n",
    "#         n_seqlength, n_features = np.shape(x_train_raw)[0], np.shape(x_train_raw)[1]\n",
    "\n",
    "    # np.shape(x_train_raw), np.shape(x_test_raw), np.shape(rbps_train), np.shape(rbps_test), np.shape(y_train), np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "140ea2d4-457d-4c42-aa89-4b8eef21c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dataset_save_name):\n",
    "    print(np.shape(y_train), np.shape(y_test))\n",
    "    print(np.shape(x_train_raw), np.shape(x_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0cefdf-b713-4d6f-9646-0f920bdeba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(50):\n",
    "#     print(len(x_test_raw[i*(-1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7dce34-f09d-4523-92f9-2970c8efda29",
   "metadata": {},
   "source": [
    "# Model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6ccfd-6754-481a-a5b2-98a7d9feaaa4",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c26b31-11f0-491d-83ef-31d0cfe4728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters\n",
    "#### Mostly unchanged\n",
    "RANDOMIZE_WEIGHTS = False \n",
    "\n",
    "MODEL_NAME = \"simecek/DNADebertaK6c\"\n",
    "TOKENIZER_NAME = \"armheb/DNA_bert_6\"\n",
    "K = 6\n",
    "STRIDE = 1\n",
    "\n",
    "# Lower batch_size and increase numbre of accumulation steps if your machine strugles with memory\n",
    "BATCH_SIZE = 64\n",
    "# Higher accumulation could help with overfitting by \"decreasing the number of update steps\"\n",
    "ACCUMULATION = 1\n",
    "\n",
    "# set\n",
    "EPOCHS = 1\n",
    "# EPOCHS = 5\n",
    "\n",
    "#### Mostly unchanged\n",
    "LEARNING_RATE = 1e-5\n",
    "RUNS = 1\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "USE_CLASS_WEIGHTS = False\n",
    "hard_nucleotide_masking = False\n",
    "\n",
    "SEED=42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# warmup_ratio = 0.05 #5 epochs (for 100 epochs total train)\n",
    "warmup_ratio = 0.20\n",
    "if(RANDOMIZE_WEIGHTS):\n",
    "    warmup_ratio = 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5dfa1f-cdf8-4dbf-9c35-9b9d6062b291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rad5cds_local.max.ds2to11_adaptive_wd0.01_13n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631aad8-ad67-4602-bb81-99e799ba2c13",
   "metadata": {},
   "source": [
    "### Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b65217ec-8c97-40a7-9c84-75fbb767a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.integrations import CometCallback\n",
    "\n",
    "\n",
    "def get_trainargs():\n",
    "    return TrainingArguments(\n",
    "        'outputs' + '/' + folder_run_id, \n",
    "        learning_rate=LEARNING_RATE, \n",
    "        warmup_ratio=warmup_ratio, \n",
    "        lr_scheduler_type='linear',\n",
    "        fp16=True,\n",
    "        per_device_train_batch_size=BATCH_SIZE, \n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=ACCUMULATION,\n",
    "        num_train_epochs=EPOCHS, \n",
    "        # max_steps=4500, # there is 17 898 Total optimization steps in 1 epoch\n",
    "        # save_steps=4500,     # Frequency of saving checkpoints\n",
    "        # logging_steps=4500,   # Logging frequency\n",
    "        # save_strategy='steps',\n",
    "        # evaluation_strategy=\"steps\", \n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\", \n",
    "        save_strategy='epoch',\n",
    "        # save_strategy='no',\n",
    "        seed=randrange(1,10001), \n",
    "        report_to='none',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_loss'\n",
    "    )\n",
    "#early stopping 5 epochs\n",
    "callbacks= [\n",
    "    EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE, early_stopping_threshold=0.001),\n",
    "    CometCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbdb0a-eeab-4524-94a4-d5d5a930d81f",
   "metadata": {},
   "source": [
    "### Get tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9133e-b670-4c4e-b920-59b9d54337e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa7e64-43a5-4820-872e-bfbcc5264743",
   "metadata": {},
   "source": [
    "### Nucleotide masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfb9a6-23e4-4106-8934-e650df20a2bc",
   "metadata": {},
   "source": [
    "- so far, it is either hard-masking or no-masking\n",
    "- there is no soft-masking possible with this \\**pretrained*\\* model because the model's alphabet contains only \"A, C, T, G, UNK\" tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319c45a-8b5a-4b3d-8221-16e61d54f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "if not os.path.exists(dataset_save_name):\n",
    "    \n",
    "    \"\".join(x_train_raw[100])\n",
    "    # x_train_raw.dtype\n",
    "\n",
    "    \"\".join(x_test_raw[0])\n",
    "\n",
    "    if not hard_nucleotide_masking:\n",
    "        # hack to unmask nucleotides\n",
    "        try:\n",
    "            x_train = np.char.upper(x_train_raw)\n",
    "        except:\n",
    "            x_train = np.char.upper(x_train_raw.astype(str))\n",
    "\n",
    "        try:\n",
    "            x_test = np.char.upper(x_test_raw)\n",
    "        except:\n",
    "            x_test = np.char.upper(x_test_raw.astype(str))\n",
    "    else:\n",
    "        # no action needed, the tokenizer will map lower case chars to UNK tokens\n",
    "        pass\n",
    "\n",
    "    \"\".join(x_train[100])\n",
    "\n",
    "    ### Tokenize\n",
    "\n",
    "    def kmers_strideK(s, k=K):\n",
    "        return [s[i:i + k] for i in range(0, len(s), k) if i + k <= len(s)]\n",
    "\n",
    "    def kmers_stride1(s, k=K):\n",
    "        return [s[i:i + k] for i in range(0, len(s)-k+1)]\n",
    "\n",
    "    if (STRIDE == 1):\n",
    "        kmers = kmers_stride1\n",
    "    else:\n",
    "        kmers = kmers_strideK\n",
    "\n",
    "    # function used for the actual tokenization\n",
    "    if(K is not None):\n",
    "        def tok_func(x): return tokenizer(\" \".join(kmers(x[\"seq\"])), truncation=True)\n",
    "    else:\n",
    "        def tok_func(x): return tokenizer(x[\"seq\"], truncation=True)\n",
    "\n",
    "    # example\n",
    "    example = tok_func({'seq': \"\".join(x_train[100])})    \n",
    "    print(example)\n",
    "    tokenizer.decode(example['input_ids'])\n",
    "\n",
    "    len(example['input_ids'])\n",
    "\n",
    "    tmp_dict_train = {}\n",
    "    # TRAIN\n",
    "    for index, (x, y) in enumerate(zip(x_train, y_train)):\n",
    "        tmp_dict_train[f\"{index}\"] = (\"train\", y, x)\n",
    "\n",
    "    tmp_dict_test = {}\n",
    "    # TEST\n",
    "    for index, (x, y) in enumerate(zip(x_test, y_test)):\n",
    "        tmp_dict_test[f\"{index}\"] = (\"test\", y, x)\n",
    "\n",
    "    df_train = pd.DataFrame.from_dict(tmp_dict_train).T.rename(columns = {0: \"dset\", 1: \"cat\", 2: \"seq\"})\n",
    "    df_test = pd.DataFrame.from_dict(tmp_dict_test).T.rename(columns = {0: \"dset\", 1: \"cat\", 2: \"seq\"})\n",
    "\n",
    "    df_train['seq'] = [''.join(map(str, l)) for l in df_train['seq']]\n",
    "    df_test['seq'] = [''.join(map(str, l)) for l in df_test['seq']]\n",
    "\n",
    "\n",
    "    ds_train = Dataset.from_pandas(df_train)\n",
    "    ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "    # tok_ds = ds.map(tok_func, batched=False, remove_columns=['__index_level_0__', 'seq'])\n",
    "    tok_ds_train = ds_train.map(tok_func, batched=False, remove_columns=['__index_level_0__', 'seq'])\n",
    "    tok_ds_train = tok_ds_train.rename_columns({'cat':'labels'})\n",
    "\n",
    "    tok_ds_test = ds_test.map(tok_func, batched=False, remove_columns=['__index_level_0__', 'seq'])\n",
    "    tok_ds_test = tok_ds_test.rename_columns({'cat':'labels'})\n",
    "\n",
    "\n",
    "    dds = DatasetDict({\n",
    "        'train': tok_ds_train,\n",
    "        'valid': [],\n",
    "        'test': tok_ds_test\n",
    "    })\n",
    "\n",
    "    train_valid_split = dds['train'].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "    dds['train']=train_valid_split['train']\n",
    "    dds['valid']=train_valid_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461dcd0b-962c-4380-9cc9-6003e4b5f04c",
   "metadata": {},
   "source": [
    "### Save or load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ba95f-00ec-4ca8-8cc2-7d3bd65d65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "if os.path.exists(dataset_save_name):\n",
    "    print(\"File exists:\", dataset_save_name)\n",
    "    dds = load_from_disk(dataset_save_name)\n",
    "else:\n",
    "    print(\"File not found:\", dataset_save_name)\n",
    "    dds.save_to_disk(dataset_save_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d90856-dd76-472a-9d9d-b0473f6ead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shorten_sequence(sample):\n",
    "#     sample['input_ids'] = sample['input_ids']\n",
    "#     return sample\n",
    "\n",
    "# updated_dataset = dds['test'].map(shorten_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe4b78c-bd11-4163-bd1b-809142b8d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(updated_dataset['input_ids'][0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445c62a-1f95-44a1-a56c-2c61edbaf5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     print(updated_dataset['input_ids'][i][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0420eca-aa1e-4058-94b1-e6a4dec72bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = tokenizer.decode(updated_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6c500-af9e-442e-8f16-6a77c6a75408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bfabad-a397-41ed-a8e2-3cc25599c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # truncation of # of front and # of back\n",
    "# def remove_lateral_tokens(seq, remove_number = 50):\n",
    "#     splited_seq = seq.split()\n",
    "    \n",
    "#     start_tok = [splited_seq.pop(0)]\n",
    "#     end_tok = [splited_seq.pop(-1)]\n",
    "#     print(start_tok, end_tok, splited_seq)\n",
    "#     print(len(splited_seq))\n",
    "    \n",
    "#     shortened = splited_seq[remove_number:][:-remove_number]\n",
    "#     start_tok.extend(shortened)\n",
    "#     start_tok.extend(end_tok)\n",
    "#     return \" \".join(start_tok)\n",
    "    \n",
    "# # print(remove_lateral_tokens(tmp))    \n",
    "# len(tokenizer.encode(remove_lateral_tokens(tmp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844847b-7590-4ad4-bc38-a92dbdd9ffd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707470ab-d5fe-4db4-bd78-33ee87b08ade",
   "metadata": {},
   "source": [
    "Function to extract dataframe metrics row from training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48094614-5cfd-4b6d-ae92-d3b2fc20c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_from_history(history, dataset_name):\n",
    "    eval_dicts = [x for x in history if 'eval_loss' in x]\n",
    "    test_dicts = [x for x in history if 'test_loss' in x]\n",
    "    test_log = test_dicts[0]\n",
    "    test_acc = test_log['test_accuracy']\n",
    "    test_f1 = test_log['test_f1']\n",
    "    test_loss = test_log['test_loss']\n",
    "    test_precision = test_log['test_precision']\n",
    "    test_recall = test_log['test_recall']\n",
    "    test_auroc_macro = test_log['test_rocauc_0_roc_auc']\n",
    "    test_auroc_weighted = test_log['test_rocauc_1_roc_auc']\n",
    "    test_pr_auc = test_log['test_pr_auc']\n",
    "    \n",
    "    \n",
    "    min_loss_dict = min(eval_dicts, key=lambda x: x['eval_loss'])\n",
    "    min_loss_epoch = min_loss_dict['epoch']\n",
    "    # max_f1_dict = max(eval_dicts, key=lambda x: x['eval_f1'])\n",
    "    # max_acc_dict = max(eval_dicts, key=lambda x: x['eval_accuracy'])\n",
    "    row = {\n",
    "        'dataset':dataset_name,\n",
    "        'test_acc':test_acc,\n",
    "        'test_f1':test_f1,\n",
    "        'test_loss':test_loss,\n",
    "        'test_precision':test_precision,\n",
    "        'test_recall':test_recall,\n",
    "        'test_auroc_macro':test_auroc_macro,\n",
    "        'test_auroc_weighted':test_auroc_weighted,\n",
    "        'test_pr_auc':test_pr_auc,\n",
    "        \n",
    "        'min_valid_loss_epoch':min_loss_epoch,\n",
    "        'min_valid_loss_log':min_loss_dict,\n",
    "        # 'max_valid_f1_log':max_f1_dict,\n",
    "        # 'max_valid_acc_log':max_acc_dict,\n",
    "    }\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5dd23-f070-40bb-9541-75a4b533e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "binary_metrics = evaluate.combine([\n",
    "    'accuracy',\n",
    "    'f1',\n",
    "    'recall',\n",
    "    'precision',\n",
    "    #Order of roc_auc matters for logging -> macro first, then weighted\n",
    "    evaluate.load('roc_auc', average='macro'),\n",
    "    evaluate.load('roc_auc', average='weighted'),\n",
    "    evaluate.load(\"Vlasta/pr_auc\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d663b-ba50-4e97-b240-89606aef038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import random, randrange\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "\n",
    "def compute_metrics_binary(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    prediction_scores = torch.nn.functional.softmax(\n",
    "        torch.from_numpy(logits).double(), dim=-1).numpy() \n",
    "    # predictions = np.argmax(logits, axis=-1) #equivalent\n",
    "    predictions = np.argmax(prediction_scores, axis=-1)\n",
    "    return binary_metrics.compute(\n",
    "        predictions=predictions, \n",
    "        references=labels, \n",
    "        prediction_scores=prediction_scores[:,1] #taking only prediction percentage for the label 1\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d346c-f858-4680-8e86-4d3fd047aaea",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08660659-9121-4e3e-95e9-62a8acf36656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom trainer for class weights in compute_loss\n",
    "from torch import nn, save\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([11.68802694, 1.0]).to('cuda'))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1)).to('cuda')\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931c245-4475-49a5-9c4b-f3ea778cc360",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"decay\"\n",
    "compute_metrics = compute_metrics_binary\n",
    "outputs = []\n",
    "\n",
    "if(USE_CLASS_WEIGHTS):\n",
    "    Trainer_to_use = CustomTrainer\n",
    "else:\n",
    "    Trainer_to_use = Trainer\n",
    "\n",
    "for _ in range(RUNS):\n",
    "    model_cls = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    if(RANDOMIZE_WEIGHTS):\n",
    "        # model_cls.init_weights() #Alternative\n",
    "        model_cls = AutoModelForSequenceClassification.from_config(model_cls.config)            \n",
    "\n",
    "    args = get_trainargs()\n",
    "    \n",
    "    trainer = Trainer_to_use(model_cls, args, train_dataset=dds['train'], eval_dataset=dds['valid'],\n",
    "                      tokenizer=tokenizer, compute_metrics=compute_metrics, \n",
    "                      callbacks=callbacks) \n",
    "        \n",
    "    trainer.train()\n",
    "    # trainer.evaluate(dds['test'], metric_key_prefix='test')\n",
    "    predictions = trainer.evaluate(dds['test'], metric_key_prefix='test')\n",
    "    training_log = get_log_from_history(trainer.state.log_history, dataset_name=dataset_name)\n",
    "    outputs.append(training_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba6ee5-7f93-4dc4-966c-8415002a2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be41c0a7-9b23-4048-a587-146406abd827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: dset. If dset are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 163476\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163476, 2) (163476,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predictions = trainer.predict(dds[\"test\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "confusion_matrix_test = metrics.confusion_matrix(predictions.label_ids, preds, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb6a54-0c53-4770-916e-05d305cd4dec",
   "metadata": {},
   "source": [
    "count of true negatives is C(0,0), false negatives is C(1,0), true positives is C(1,1) and false positives is C(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5aa710a-5666-440b-8570-105c99180801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40a2bbdf-297b-475a-8c39-d4b16345066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81738 81738\n",
      "test\n",
      "precision  0.6370300893733087\n",
      "recall  0.7229073380802075\n",
      "specificity  0.5880985588098558\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix_test.ravel()\n",
    "print(tp + fn, tn + fp)\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print('test')\n",
    "print('precision ', precision)\n",
    "print('recall ', recall)\n",
    "print('specificity ', specificity)\n",
    "\n",
    "confusion_matrix_test = confusion_matrix_test.tolist()\n",
    "confusion_matrix_test.append({'precision': precision, 'recall': recall, 'specificity': specificity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c916491-670f-4833-8784-211d4c71ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.append({'confusion_matrix_test' : confusion_matrix_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20655475-6015-4970-aad9-a4c29c9e76dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataset': 'decay',\n",
       "  'test_acc': 0.6555029484450317,\n",
       "  'test_f1': 0.6772572280008023,\n",
       "  'test_loss': 0.622995913028717,\n",
       "  'test_precision': 0.6370300893733087,\n",
       "  'test_recall': 0.7229073380802075,\n",
       "  'test_auroc_macro': 0.7104757603468905,\n",
       "  'test_auroc_weighted': 0.7104757603468905,\n",
       "  'test_pr_auc': 0.6831714813764451,\n",
       "  'min_valid_loss_epoch': 1.0,\n",
       "  'min_valid_loss_log': {'eval_loss': 0.6176930665969849,\n",
       "   'eval_accuracy': 0.6597058289449791,\n",
       "   'eval_f1': 0.6827823278948945,\n",
       "   'eval_recall': 0.7320996013989431,\n",
       "   'eval_precision': 0.6396901399859709,\n",
       "   'eval_rocauc_0_roc_auc': 0.7176280425958544,\n",
       "   'eval_rocauc_1_roc_auc': 0.7176280425958544,\n",
       "   'eval_pr_auc': 0.6923296338922672,\n",
       "   'eval_runtime': 46.899,\n",
       "   'eval_samples_per_second': 6105.97,\n",
       "   'eval_steps_per_second': 95.418,\n",
       "   'epoch': 1.0,\n",
       "   'step': 17898}},\n",
       " {'confusion_matrix_test': [[48070, 33668],\n",
       "   [22649, 59089],\n",
       "   {'precision': 0.6370300893733087,\n",
       "    'recall': 0.7229073380802075,\n",
       "    'specificity': 0.5880985588098558}]}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b69eeeff-4114-4dec-8da2-50938d502255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(folder_run_id + '_log.txt', 'w+') as f:\n",
    "    f.write(json.dumps(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d4d2ca4-ac09-4cc8-a43a-eb471f236a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model_cls, folder_run_id + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8dcb3de4-e0ac-4ef0-9ed2-bd7cdb1766b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rad5cds_local.max.ds2to11_adaptive_wd0.01_13n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1efa7f6d-60d4-4c4b-b58b-08fa5bf577bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2004f64-2f65-4c50-9793-1d89358b5c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decay2]",
   "language": "python",
   "name": "conda-env-decay2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
